diff --git a/.changeset/busy-zebras-begin.md b/.changeset/busy-zebras-begin.md
new file mode 100644
index 0000000000..bb68b9cd66
--- /dev/null
+++ b/.changeset/busy-zebras-begin.md
@@ -0,0 +1,19 @@
+---
+"@biomejs/biome": patch
+---
+
+The HTML formatter has been updated to match Prettier 3.7's behavior for handling `<iframe>`'s `allow` attribute.
+
+```diff
+- <iframe allow="layout-animations 'none'; unoptimized-images 'none'; oversized-images 'none'; sync-script 'none'; sync-xhr 'none'; unsized-media 'none';"></iframe>
++ <iframe
++ 	allow="
++ 		layout-animations 'none';
++ 		unoptimized-images 'none';
++ 		oversized-images 'none';
++ 		sync-script 'none';
++ 		sync-xhr 'none';
++ 		unsized-media 'none';
++ 	"
++ ></iframe>
+```
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index ff0df277ce..c2795534d9 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -444,23 +444,17 @@ When choosing `minor` or `major`, make sure your PR targets the `next` branch in
 
 The description of the changeset should follow the these guidelines:
 
-- Our changesets should be about **user-facing changes**. Internal changes don't
-  need changesets. For example, if you refactored some code, but the user-facing
-  behavior didn't change, you don't need a changeset.
-- **Be concise and clear.** Changesets are not documentation, and they are not test cases.
-  They should give a **quick overview** of what changed, allowing the reader to dig deeper if they want to. A good changeset is usually between 1 and 3 sentences long. **Longer changesets indicate to the user that they should pay more attention to the change, at least if it concerns a feature that is relevant to them.** Consider the impact of your change on the user when writing the changeset. The key is to provide just enough information for the user to understand the change without overwhelming them with details.
-  - For a *new* lint rule, show an example of an invalid case in an inline code snippet for simple things or a code block for more complex examples. If it _really_ helps demonstrate the rule, you can also show a valid case.
-  - For a *change in an existing rule*, clearly demonstrate what is now considered invalid that wasn't before, or vice versa. If it helps communicate the change greatly, show both invalid and valid cases.
-  - For a *formatter change*, show an example of the formatting change using a `diff` code block.
-  - For parser changes, a brief inline example of what can now be parsed that couldn't before (or vice versa) is usually sufficient. If formatting on multiple lines improves clarity, use a code block.
-- **Use past tense when describing what you did**, e.g. "Added new feature", "Fixed edge case".
-- **Use the present tense when describing Biome behavior**, e.g. "Biome now supports ...".
+- Our changesets should be about _user-facing_ changes. Internal changes don't
+  need changesets.
+- Use the past tense when describing what you did, e.g. "Added new feature", "Fixed edge case".
+- Use the present tense when describing Biome behavior, e.g. "Biome now supports ...".
 - If you fixed a bug, please start with a link to the issue, e.g. "Fixed [#4444](https://github.com/biomejs/biome/issues/4444): ...".
-- If you reference a rule, please add the link to the rule on the website, e.g. "Added the rule [`useAwesomeThing`](https://biomejs.dev/linter/rules/use-awesome-thing/)" (even if the website isn't updated at the time of writing, it will once the PR is merged).
+- If you reference a rule, please add the link to the rule on the website, e.g. "Added the rule [`useAwesomeThing`](https://biomejs.dev/linter/rules/use-awesome-thing/)" (even if the website isn't updated yet, the URL is pretty predictable...).
 - Similarly, if you reference an assist, please add the link to the assist on the website, e.g. "Added the assist [`awesomeAction`](https://biomejs.dev/assist/actions/awesome-action/)".
+- Whenever applicable, add a code block to show your new changes. For example, for a new rule you should show an invalid case, while for the formatter you should show how the new formatting changes, and so on.
 - End every sentence with a full stop (`.`).
 
-If in doubt, take a look at existing changesets, or the most recent entries in `CHANGELOG.md`, and use your best judgement.
+If in doubt, take a look at existing or past changesets.
 
 ### Documentation
 
diff --git a/crates/biome_html_formatter/src/html/auxiliary/attribute.rs b/crates/biome_html_formatter/src/html/auxiliary/attribute.rs
index dd4842d22b..94fc9dbc2d 100644
--- a/crates/biome_html_formatter/src/html/auxiliary/attribute.rs
+++ b/crates/biome_html_formatter/src/html/auxiliary/attribute.rs
@@ -1,4 +1,10 @@
-use crate::{html::auxiliary::attribute_name::FormatHtmlAttributeNameOptions, prelude::*};
+use crate::{
+    html::auxiliary::{
+        attribute_initializer_clause::FormatHtmlAttributeInitializerClauseOptions,
+        attribute_name::FormatHtmlAttributeNameOptions,
+    },
+    prelude::*,
+};
 use biome_formatter::{FormatRuleWithOptions, write};
 use biome_html_syntax::{HtmlAttribute, HtmlAttributeFields, HtmlTagName};
 
@@ -35,13 +41,27 @@ impl FormatNodeRule<HtmlAttribute> for FormatHtmlAttribute {
 
         write!(
             f,
-            [
-                name.format()?.with_options(FormatHtmlAttributeNameOptions {
-                    is_canonical_html_element: self.is_canonical_html_element,
-                    tag_name: self.tag_name.clone(),
-                }),
-                initializer.format()
-            ]
-        )
+            [name.format()?.with_options(FormatHtmlAttributeNameOptions {
+                is_canonical_html_element: self.is_canonical_html_element,
+                tag_name: self.tag_name.clone(),
+            }),]
+        )?;
+
+        if let Some(initializer) = initializer.as_ref() {
+            write!(
+                f,
+                [initializer
+                    .format()
+                    .with_options(FormatHtmlAttributeInitializerClauseOptions {
+                        tag_name: self
+                            .tag_name
+                            .as_ref()
+                            .and_then(|name| name.token_text_trimmed()),
+                        attribute_name: name.ok().and_then(|name| name.token_text_trimmed()),
+                    })]
+            )?;
+        }
+
+        Ok(())
     }
 }
diff --git a/crates/biome_html_formatter/src/html/auxiliary/attribute_initializer_clause.rs b/crates/biome_html_formatter/src/html/auxiliary/attribute_initializer_clause.rs
index 77e6a0941b..9eaafe4fc4 100644
--- a/crates/biome_html_formatter/src/html/auxiliary/attribute_initializer_clause.rs
+++ b/crates/biome_html_formatter/src/html/auxiliary/attribute_initializer_clause.rs
@@ -1,8 +1,39 @@
+use std::fmt::Debug;
+
 use crate::prelude::*;
-use biome_formatter::write;
+use biome_formatter::{CstFormatContext, FormatRuleWithOptions, write};
 use biome_html_syntax::{HtmlAttributeInitializerClause, HtmlAttributeInitializerClauseFields};
+use biome_rowan::TokenText;
+
 #[derive(Debug, Clone, Default)]
-pub(crate) struct FormatHtmlAttributeInitializerClause;
+pub(crate) struct FormatHtmlAttributeInitializerClause {
+    /// The name of the tag this attribute belongs to.
+    pub tag_name: Option<TokenText>,
+
+    /// The name of the attribute this initializer clause belongs to.
+    pub attribute_name: Option<TokenText>,
+}
+
+pub(crate) struct FormatHtmlAttributeInitializerClauseOptions {
+    /// The name of the tag this attribute belongs to.
+    pub tag_name: Option<TokenText>,
+
+    /// The name of the attribute this initializer clause belongs to.
+    pub attribute_name: Option<TokenText>,
+}
+
+impl FormatRuleWithOptions<HtmlAttributeInitializerClause>
+    for FormatHtmlAttributeInitializerClause
+{
+    type Options = FormatHtmlAttributeInitializerClauseOptions;
+
+    fn with_options(mut self, options: Self::Options) -> Self {
+        self.tag_name = options.tag_name;
+        self.attribute_name = options.attribute_name;
+        self
+    }
+}
+
 impl FormatNodeRule<HtmlAttributeInitializerClause> for FormatHtmlAttributeInitializerClause {
     fn fmt_fields(
         &self,
@@ -11,6 +42,68 @@ impl FormatNodeRule<HtmlAttributeInitializerClause> for FormatHtmlAttributeIniti
     ) -> FormatResult<()> {
         let HtmlAttributeInitializerClauseFields { eq_token, value } = node.as_fields();
 
-        write![f, [eq_token.format(), value.format()]]
+        // We currently only have special formatting for when the value is a string.
+        if let Some(html_string) = value.as_ref()?.as_html_string()
+            && !f.context().comments().is_suppressed(html_string.syntax())
+        {
+            match (self.tag_name.as_deref(), self.attribute_name.as_deref()) {
+                // Prettier 3.7 handles allow attribute on iframes specially by splitting the
+                // value on semicolons and formatting it like a list, breaking if its too long, or leaving it on one line if it fits in the line width.
+                // It also trims whitespace around each item, and removes empty items.
+                //
+                // Before:
+                // ```html
+                // <iframe allow="    camera; ;    ;  accelerometer;"></iframe>
+                // ```
+                //
+                // After:
+                // ```html
+                // <iframe allow="camera; accelerometer"></iframe>
+                // ```
+                (Some("iframe"), Some("allow")) => {
+                    let content = html_string.inner_string_text()?;
+                    let value_token = html_string.value_token()?;
+
+                    struct JoinWithSemicolon;
+                    impl Format<HtmlFormatContext> for JoinWithSemicolon {
+                        fn fmt(&self, f: &mut HtmlFormatter) -> FormatResult<()> {
+                            write!(f, [token(";"), soft_line_break_or_space()])
+                        }
+                    }
+
+                    write!(
+                        f,
+                        [
+                            eq_token.format(),
+                            format_removed(&value_token),
+                            token("\""),
+                            group(&soft_block_indent(&format_with(|f| {
+                                let items = content
+                                    .split(';')
+                                    .map(TokenText::trim_token)
+                                    .filter(|s| !s.is_empty())
+                                    .collect::<Vec<_>>();
+                                f.join_with(JoinWithSemicolon)
+                                    .entries(items.into_iter().map(|item| {
+                                        located_token_text(
+                                            &value_token,
+                                            item.source_range(value_token.text_range()),
+                                        )
+                                    }))
+                                    .finish()?;
+                                write!(f, [if_group_breaks(&token(";"))])?;
+                                Ok(())
+                            }))),
+                            token("\"")
+                        ]
+                    )
+                }
+                _ => {
+                    write!(f, [eq_token.format(), value.format()])
+                }
+            }
+        } else {
+            write!(f, [eq_token.format(), value.format()])
+        }
     }
 }
diff --git a/crates/biome_html_syntax/src/attr_ext.rs b/crates/biome_html_syntax/src/attr_ext.rs
index a28ad614c2..03ac73ef25 100644
--- a/crates/biome_html_syntax/src/attr_ext.rs
+++ b/crates/biome_html_syntax/src/attr_ext.rs
@@ -1,8 +1,8 @@
 use crate::{
     AnyHtmlAttribute, AnyHtmlAttributeInitializer, HtmlAttribute, HtmlAttributeList,
-    inner_string_text,
+    HtmlAttributeName, inner_string_text,
 };
-use biome_rowan::{AstNodeList, Text};
+use biome_rowan::{AstNodeList, Text, TokenText};
 
 impl AnyHtmlAttributeInitializer {
     /// Returns the string value of the attribute, if available, without quotes.
@@ -32,3 +32,17 @@ impl HtmlAttributeList {
         })
     }
 }
+
+impl HtmlAttributeName {
+    /// Returns the token text of the attribute name.
+    pub fn token_text(&self) -> Option<TokenText> {
+        self.value_token().ok().map(|token| token.token_text())
+    }
+
+    /// Returns the trimmed token text of the attribute name.
+    pub fn token_text_trimmed(&self) -> Option<TokenText> {
+        self.value_token()
+            .ok()
+            .map(|token| token.token_text_trimmed())
+    }
+}
diff --git a/crates/biome_html_syntax/src/element_ext.rs b/crates/biome_html_syntax/src/element_ext.rs
index 05942c1a54..b9218f7a7f 100644
--- a/crates/biome_html_syntax/src/element_ext.rs
+++ b/crates/biome_html_syntax/src/element_ext.rs
@@ -1,6 +1,6 @@
 use crate::{
     AnyHtmlElement, AstroEmbeddedContent, HtmlAttribute, HtmlElement, HtmlEmbeddedContent,
-    HtmlSelfClosingElement, HtmlSyntaxToken, ScriptType, inner_string_text,
+    HtmlSelfClosingElement, HtmlSyntaxToken, HtmlTagName, ScriptType, inner_string_text,
 };
 use biome_rowan::{AstNodeList, SyntaxResult, TokenText, declare_node_union};
 
@@ -206,6 +206,20 @@ impl HtmlElement {
     }
 }
 
+impl HtmlTagName {
+    /// Returns the token text of the attribute name.
+    pub fn token_text(&self) -> Option<TokenText> {
+        self.value_token().ok().map(|token| token.token_text())
+    }
+
+    /// Returns the trimmed token text of the attribute name.
+    pub fn token_text_trimmed(&self) -> Option<TokenText> {
+        self.value_token()
+            .ok()
+            .map(|token| token.token_text_trimmed())
+    }
+}
+
 #[cfg(test)]
 mod tests {
     use biome_html_factory::syntax::HtmlElement;
diff --git a/crates/biome_rowan/src/token_text.rs b/crates/biome_rowan/src/token_text.rs
index b4c388aa0a..441b6c25a6 100644
--- a/crates/biome_rowan/src/token_text.rs
+++ b/crates/biome_rowan/src/token_text.rs
@@ -57,6 +57,19 @@ impl TokenText {
         self.range.is_empty()
     }
 
+    /// Returns the range of this text relative to the token.
+    pub fn relative_range(&self) -> TextRange {
+        self.range
+    }
+
+    /// Given the range of the token in the source file, returns the range of this text in the source file.
+    pub fn source_range(&self, whole_token_range: TextRange) -> TextRange {
+        TextRange::new(
+            whole_token_range.start() + self.range.start(),
+            whole_token_range.start() + self.range.end(),
+        )
+    }
+
     /// Returns a subslice of the text.
     /// `range.end()` must be lower or equal to `self.len()`
     pub fn slice(mut self, range: TextRange) -> Self {
@@ -72,6 +85,49 @@ impl TokenText {
     pub fn text(&self) -> &str {
         &self.token.text()[self.range]
     }
+
+    /// Returns a view into this `TokenText` with leading and trailing
+    /// Unicode whitespace removed, without allocating.
+    ///
+    /// Whitespace is determined via `char::is_whitespace`.
+    /// The returned `TokenText` references the same underlying token with
+    /// an adjusted range.
+    #[inline]
+    pub fn trim_token(self) -> Self {
+        let s = self.text();
+
+        // Compute leading whitespace (in bytes)
+        let mut start_bytes = 0;
+        for (idx, ch) in s.char_indices() {
+            if ch.is_whitespace() {
+                start_bytes = idx + ch.len_utf8();
+            } else {
+                break;
+            }
+        }
+
+        // Compute trailing whitespace (in bytes)
+        let mut end_bytes = s.len();
+        for (idx, ch) in s.char_indices().rev() {
+            if ch.is_whitespace() {
+                end_bytes = idx;
+            } else {
+                break;
+            }
+        }
+
+        // Create a slice of the current view; range is relative to self.range
+        // Clamp to avoid start > end when the entire slice is whitespace.
+        let (start_u, end_u) = if end_bytes < start_bytes {
+            (start_bytes, start_bytes)
+        } else {
+            (start_bytes, end_bytes)
+        };
+        let start = TextSize::from(start_u as u32);
+        let end = TextSize::from(end_u as u32);
+        let range = TextRange::new(start, end);
+        self.slice(range)
+    }
 }
 
 impl Deref for TokenText {
@@ -123,3 +179,211 @@ impl Borrow<str> for TokenText {
         self.text()
     }
 }
+
+impl TokenText {
+    /// Returns an iterator over substrings of this `TokenText`, separated by
+    /// occurrences of the given pattern.
+    ///
+    /// The returned items are `TokenText` values that reference the same underlying
+    /// token, with adjusted ranges. This avoids copying or allocating new strings.
+    ///
+    /// Supported patterns:
+    /// - `char`
+    /// - `&str` (empty patterns are treated as "no-op", yielding the original text as a single item)
+    pub fn split<P>(&self, pattern: P) -> TokenTextSplit<P>
+    where
+        P: TokenTextPattern,
+    {
+        TokenTextSplit {
+            token: self.token.clone(),
+            remaining: self.range,
+            pattern,
+            finished: false,
+        }
+    }
+}
+
+/// A trait representing a pattern that can be used to split a `TokenText`.
+pub trait TokenTextPattern {
+    /// Finds the next match of this pattern in `haystack`,
+    /// returning the byte range `(start, end)` of the match.
+    fn find(&self, haystack: &str) -> Option<(usize, usize)>;
+}
+
+impl TokenTextPattern for char {
+    fn find(&self, haystack: &str) -> Option<(usize, usize)> {
+        haystack
+            .find(*self)
+            .map(|start| (start, start + self.len_utf8()))
+    }
+}
+
+impl TokenTextPattern for &str {
+    fn find(&self, haystack: &str) -> Option<(usize, usize)> {
+        // Treat empty pattern as "no-op" to avoid infinite splitting behavior.
+        if self.is_empty() {
+            return None;
+        }
+        haystack.find(self).map(|start| (start, start + self.len()))
+    }
+}
+
+/// Iterator over the substrings of a `TokenText` separated by a pattern.
+pub struct TokenTextSplit<P> {
+    token: GreenToken,
+    remaining: TextRange,
+    pattern: P,
+    finished: bool,
+}
+
+impl<P: TokenTextPattern> Iterator for TokenTextSplit<P> {
+    type Item = TokenText;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.finished {
+            return None;
+        }
+
+        let hay = &self.token.text()[self.remaining];
+
+        if let Some((start, end)) = self.pattern.find(hay) {
+            let piece_len = TextSize::from(start as u32);
+            let piece_range = TextRange::at(self.remaining.start(), piece_len);
+
+            // Advance remaining past the matched delimiter.
+            let skip = TextSize::from(end as u32);
+            let new_start = self.remaining.start() + skip;
+            self.remaining = TextRange::new(new_start, self.remaining.end());
+
+            Some(TokenText::with_range(self.token.clone(), piece_range))
+        } else {
+            // No more delimiters; yield the remaining part and finish.
+            self.finished = true;
+            let piece = TokenText::with_range(self.token.clone(), self.remaining);
+            Some(piece)
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::RawSyntaxKind;
+
+    fn tt(text: &str) -> TokenText {
+        TokenText::new_raw(RawSyntaxKind(0), text)
+    }
+
+    #[test]
+    fn split_char_basic() {
+        let t = tt("a,b,c");
+        let parts: Vec<String> = t.split(',').map(|p| p.text().to_string()).collect();
+        assert_eq!(parts, vec!["a", "b", "c"]);
+    }
+
+    #[test]
+    fn split_char_adjacent_delims() {
+        let t = tt("a,,b,,,c,");
+        let parts: Vec<String> = t.split(',').map(|p| p.text().to_string()).collect();
+        assert_eq!(parts, vec!["a", "", "b", "", "", "c", ""]);
+    }
+
+    #[test]
+    fn split_char_leading_trailing() {
+        let t = tt(",a,b,");
+        let parts: Vec<String> = t.split(',').map(|p| p.text().to_string()).collect();
+        assert_eq!(parts, vec!["", "a", "b", ""]);
+    }
+
+    #[test]
+    fn split_char_no_match() {
+        let t = tt("abc");
+        let parts: Vec<String> = t.split(',').map(|p| p.text().to_string()).collect();
+        assert_eq!(parts, vec!["abc"]);
+    }
+
+    #[test]
+    fn split_str_basic() {
+        let t = tt("foo::bar::baz");
+        let parts: Vec<String> = t.split("::").map(|p| p.text().to_string()).collect();
+        assert_eq!(parts, vec!["foo", "bar", "baz"]);
+    }
+
+    #[test]
+    fn split_str_adjacent_delims() {
+        let t = tt("a--b----c-");
+        let parts: Vec<String> = t.split("--").map(|p| p.text().to_string()).collect();
+        // "a--b----c-" -> ["a", "b", "", "c-"]
+        assert_eq!(parts, vec!["a", "b", "", "c-"]);
+    }
+
+    #[test]
+    fn split_str_leading_trailing() {
+        let t = tt("::a::b::");
+        let parts: Vec<String> = t.split("::").map(|p| p.text().to_string()).collect();
+        assert_eq!(parts, vec!["", "a", "b", ""]);
+    }
+
+    #[test]
+    fn split_str_empty_pattern_is_noop() {
+        let t = tt("abc");
+        let parts: Vec<String> = t.split("").map(|p| p.text().to_string()).collect();
+        // We chose to treat empty pattern as no-op: single item
+        assert_eq!(parts, vec!["abc"]);
+    }
+
+    #[test]
+    fn split_preserves_send_sync_semantics_by_referencing_same_token() {
+        let t = tt("a,b");
+        let mut it = t.split(',');
+        let first = it.next().unwrap();
+        let second = it.next().unwrap();
+        assert_eq!(first.text(), "a");
+        assert_eq!(second.text(), "b");
+        // Ensure both items are views into the same underlying token by comparing against slices of the original text
+        let original = t.text().to_string();
+        assert_eq!(&original[0..1], first.text());
+        assert_eq!(&original[2..3], second.text());
+    }
+
+    #[test]
+    fn split_iter_exhaustion() {
+        let t = tt("x|y");
+        let mut it = t.split('|');
+        assert!(it.next().is_some());
+        assert!(it.next().is_some());
+        assert!(it.next().is_none());
+        assert!(it.next().is_none());
+    }
+
+    #[test]
+    fn trim_basic_whitespace() {
+        let t = tt("  \t\nhello world\n\t  ");
+        let trimmed = t.trim_token();
+        assert_eq!(trimmed.text(), "hello world");
+    }
+
+    #[test]
+    fn trim_all_whitespace_becomes_empty() {
+        let t = tt(" \t\n\r ");
+        let trimmed = t.trim_token();
+        assert_eq!(trimmed.text(), "");
+        assert!(trimmed.is_empty());
+        assert_eq!(trimmed.len(), TextSize::from(0));
+    }
+
+    #[test]
+    fn trim_no_whitespace_unchanged() {
+        let t = tt("nowhitespace");
+        let trimmed = t.clone().trim_token();
+        assert_eq!(trimmed.text(), "nowhitespace");
+        assert_eq!(trimmed.len(), t.len());
+    }
+
+    #[test]
+    fn trim_preserves_internal_whitespace() {
+        let t = tt("  a  b   c  ");
+        let trimmed = t.trim_token();
+        assert_eq!(trimmed.text(), "a  b   c");
+    }
+}
